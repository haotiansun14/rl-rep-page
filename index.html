<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>RL-REP: Representation-based Reinforcement Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.9em; font-weight: bold;">RL-REP<br><span style="font-size: 0.8em; font-weight: normal;">Representation-based Reinforcement Learning</span></h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                  </div>
                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">A series of work with</span>
                    <span class="eql-cntrb"><br>Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Mengjiao Yang, Joseph E. Gonzalez, <br>Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, Bo Dai</span>
                    </span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2212.08765" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>LV-REP Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2207.07150" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>CTRL Paper</span>
                  </a>
                </span>

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2311.12244" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>&mu;LV-Rep Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.12244" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Diff-SR Paper</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/haotiansun14/rl-rep" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Repo</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="text-align: left;">   
    <h4 style="font-weight: normal;">
      When faced with the intricate trade-off among modeling, exploration, and planning, one might wonder the following question:<br>
      <br><div style="text-align: center;"><em>What is a practical algorithm with rigorous guarantee to achieve such balance?</em></div>
      <br>To tackle this problem, we aim to exploit model-based information to provide a reasonable linear function space for model-free planning and exploration.
    </h3>
  </div>
</section>


<style>
    /* Updated styles for a more narrow Table of Contents */
    .table-of-contents {
        border: 1px solid #ccc;
        border-radius: 10px;
        padding: 20px;
        margin: 20px auto;
        width: 800px; /* Fixed width */
        max-width: 100%; /* Ensures TOC does not exceed the viewport width */
        background-color: #f8f8f8;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        font-family: Arial, sans-serif;
    }
    .table-of-contents h2,
    .abstract-box h2 {
        color: #333;
        font-size: 24px;
        text-align: center;
        margin-bottom: 20px;
    }
    .table-of-contents ul,
    .abstract-box ul {
        list-style: none;
        padding: 0;
        margin: 0;
    }
    .table-of-contents li,
    .abstract-box li {
        margin-bottom: 10px;
        font-size: 16px;
    }
    .table-of-contents a,
    .abstract-box a {
        text-decoration: none;
        color: #007bff;
        font-weight: bold;
        transition: color 0.3s ease, text-decoration 0.3s ease;
    }
    .table-of-contents a:hover,
    .abstract-box a:hover {
        color: #0056b3;
        text-decoration: underline;
    }


  .abstract-box {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin: 20px auto;
            max-width: 80%;
            background-color: #f9f9f9;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            font-family: Arial, sans-serif;
        }
        
</style>

<div class="table-of-contents">
<h2 class="title" >Table of Contents</h2>
<ul>
  <li><a href="#LV-REP">I. LV-REP: Latent Variable Representation for Reinforcement Learning</a></li>
  <li><a href="#CTRL">II. CTRL: Making Linear MDPs Practical via Contrastive Representation Learning</a></li>
  <li><a href="#muLV-Rep">III. Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning</a></li>
  <li><a href="#Diff-SR">IV. Diffusion Spectral Representation for Reinforcement Learning</a></li>
</ul>
</div>
    
  

<section class="section" id="LV-REP">
<div class="container is-max-desktop content">   
  <h2 class="title"><strong>I. LV-REP</strong>: <span style="font-weight: normal;">Latent Variable Representation for Reinforcement Learning</span></h2>
  <!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Deep latent variable models have achieved significant empirical successes in model-based reinforcement learning (RL) due to their expressiveness in modeling complex transition dynamics. On the other hand, it remains unclear theoretically and empirically how latent variable models may facilitate learning, planning, and exploration to improve the sample efficiency of RL. In this paper, we provide a representation view of the latent variable models for state-action value functions, which allows both tractable variational learning algorithm and effective implementation of the optimism/pessimism principle in the face of uncertainty for exploration. In particular, we propose a computationally efficient planning algorithm with UCB exploration by incorporating kernel embeddings of latent variable models. Theoretically, we establish the sample complexity of the proposed approach in the online and offline settings. Empirically, we demonstrate superior performance over current state-of-the-art algorithms across various benchmarks.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Introduction</h2>
    For both model-free and model-based algorithms, there has been insufficient work considering both statistical and computation tractability and efficiency in terms of learning, planning, and exploration in a unified and coherent perspective for algorithm design. This raises the question:
    
    <div style="text-align: center; padding: 10px; margin-top: 20px; margin-bottom: 20px;" class="section hero is-light">
      <em>Is there a way to design a <strong>provable</strong> and <strong>practical</strong> algorithm that can effectively address both the statistical and computational challenges of Reinforcement Learning (RL)?</em>
    </div>
</div>
Here, by “provable” we mean the statistical complexity of the algorithm can be rigorously characterized without explicit dependence on the number of states but instead the fundamental complexity of the parameterized representation space; while by “practical” we mean the learning, planning, and exploration components in the algorithm are computationally tractable and can be implemented in real-world scenarios.
<p>
This work provides an affirmative answer to the question above by establishing the representation view of latent variable dynamics models through a connection to linear MDPs. Such a connection immediately provides a computationally tractable approach to planning and exploration in the linear space constructed by the flexible deep latent variable model. Such a latent variable model view also provides a variational learning method that remedies the intractability of MLE for general linear MDPs. Our main contributions consist of the following:
<ul>
  <li>We establish the representation view of latent variable dynamics models in RL, which naturally induces Latent Variable Representation (LV-Rep) for linearly representing the state-action value function, and paves the way for a practical variational method for representation learning;</li>
  <li>We provide computation efficient algorithms to implement the principle of optimism and pessimism in the face of uncertainty with the learned LV-Rep for online and offline RL;</li>
  <li>We theoretically analyze the sample complexity of LV-Rep in both online and offline settings, which reveals the essential complexity beyond the cardinality of the latent variable;</li>
  <li>We empirically demonstrate LV-Rep outperforms the state-of-the-art model-based and model-free RL algorithms on several RL benchmarks.</li>
</ul>

    <br>

    <h2 class="title">Experiments</h2>
    <br>
    We extensively test our algorithm on the Mujoco and DeepMind Control Suite .      
    
    <h3 class="title">Dense-Reward Mujoco Benchmarks</h3>
    <img src="static/images/lvrep-table1.png" alt="MY ALT TEXT">
    <br>
    This table presents all experiment results, where all results are averaged over 4 random seeds. 
    In practice, we found LV-Rep-C provides comparable or better performance, so that we report its result for LV-Rep in the table. 
    We present the best model-based RL performance for comparison. 
<p> The results clearly show that LV-Rep provides significantly better or comparable performance compared to all model-based algorithms. 
    In particular, in the most challenging domains such as Walker and Ant, most model-based methods completely fail the task, while LV-Rep achieves the state-of-the-art performance in contrast. 
 Furthermore, LV-Rep shows dominant performance in all domains comparing to two representative representation learning based RL methods, Deep Successor Feature (DeepSF) and SPEDE. 
    LV-Rep also achieves better performance than the strongest model-free algorithm SAC in most challenging domains except Humanoid.

    <img src="static/images/lvrep-figure1.png" alt="MY ALT TEXT">
    Finally, this figure provides learning curves of LV-Rep-C and LV-Rep-D in comparison to SAC, which clearly shows that comparing to the SOTA (state-of-the-art) model-free baseline SAC, LV-Rep enjoys great sample efficiency in these tasks.


    <h3 class="title">Sparse-Reward DeepMind Control Suite</h3>
    <img src="static/images/lvrep-table2.png" alt="MY ALT TEXT">
    <br>
    We compare all algorithms after running 200K environment steps across 4 random seeds. Results are presented in the table above. We report the result of LV-Rep-C for LV-Rep as it gives better empirical performance. We can clearly observe that LV-Rep dominates the performance across all domains. In relatively dense-reward problems, cheetah-run and walker-run, LV-Rep outperforms all baselines by a large margin. Remarkably, for sparser reward problems, hopper-hop and humanoid-run, LV-Rep provides reasonable results while other methods do not even start learning. 
    <br>
    <img src="static/images/lvrep-figure2.png" alt="MY ALT TEXT">
    <br>
    We also plot the learning curves of LV-Rep with all competitors in the figure above. This shows that LV-Rep outperforms other baselines in terms of both sample efficiency and final performance.
</section>
</section>

<section class="section" id="CTRL">
  <div class="container is-max-desktop content">   
    <h2 class="title"><strong>II. CTRL</strong>: <span style="font-weight: normal;">Making Linear MDPs Practical via Contrastive Representation Learning</span></h2>
    <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              It is common to address the curse of dimensionality in Markov decision processes (MDPs) by exploiting low-rank representations. This motivates much of the recent theoretical study on linear MDPs. However, most approaches require a given representation under unrealistic assumptions about the normalization of the decomposition or introduce unresolved computational challenges in practice. Instead, we consider an alternative definition of linear MDPs that automatically ensures normalization while allowing efficient representation learning via contrastive estimation. The framework also admits confidence-adjusted index algorithms, enabling an efficient and principled approach to incorporating optimism or pessimism in the face of uncertainty. To the best of our knowledge, this provides the first practical representation learning method for linear MDPs that achieves both strong theoretical guarantees and empirical performance. Theoretically, we prove that the proposed algorithm is sample efficient in both the online and offline settings. Empirically, we demonstrate superior performance over existing state-of-the-art model-based and model-free algorithms on several benchmarks.          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Introduction</h2>
      <div class="content">
        <p>We address the lingering computational efficiency issues in representation learning for linear MDPs. We provide solutions to the aforementioned difficulties, bridge the gap between theory and practice, and eventually establish a sound yet practical foundation for learning in linear MDPs. More specifically:</p>
        <ul>
            <li>We clarify the importance of the normalization condition and propose a variant of linear MDPs wherein it is easy to enforce normalization;</li>
            <li>We develop the ConTrastive Representation Learning algorithm, CTRL, that can implicitly satisfy the density requirement;</li>
            <li>We incorporate the representation learning techniques in optimistic and pessimistic confidence-adjusted RL algorithms, CTRL-UCB and CTRL-LCB, for both online and offline RL problems, and establish their sample complexities respectively;</li>
            <li>We conduct a comprehensive comparison to existing state-of-the-art model-based and model-free RL algorithms on several benchmarks for the online and offline settings, demonstrating superior empirical performance of the proposed CTRL.</li>
        </ul>
    </div>
    
    <h2 class="title">Experiments</h2>
    <br>
    We show that CTRL-UCB achieves state-of-the-art performance in MuJoCo and DeepMind Control Suite benchmarks, compared to both model-based (e.g., PETS, ME-TRPO) and model-free RL algorithms (e.g., SAC).    
    <h3 class="title">Dense-Reward Mujoco Benchmarks</h3>
    <img src="static/images/ctrl-table1.png" alt="MY ALT TEXT">
    <br>
    <p>In the table above, we can see that CTRL-UCBL achieves the state-of-the-art (SoTA) results among all model-based RL algorithms and significantly improves the prior baselines. Despite the improvement in terms of final return, prior model-based RL methods can hardly learn the behaviors such as walking and moving the ant in these two tasks. By contrast, CTRL-UCBL learns the two behaviors successfully.</p>
    <p>We also compare our algorithm with the SoTA model-free RL method SAC. Our method achieves comparable, sometimes better performance in most of the tasks. Lastly, comparing with two representation learning baselines: DeepSF and SPEDE, CTRL-UCBL also shows dominant performance. However, the performance of CTRL-UCBL is not satisfactory in Humanoid and Hopper environments. It is potentially due to their hard-exploration nature and the linear parametrized Q failed to capture the bonus for exploration.</p>
    

    <h3 class="title">Sparse-Reward DeepMind Control Suite</h3>
    <img src="static/images/ctrl-table2.png" alt="MY ALT TEXT">
    <br>
    <p>To better understand how CTRL performs in a sparse-reward setting, we conduct experiments on the DeepMind Control Suite. We compare CTRL-UCBM with model-free RL methods like SAC (including 2-layer MLP and 3-layer MLP as critic network) and PPO (3-layer of MLP as value network). In addition, we also compare with the representation learning method DeepSF. 
    <p>CTRL-UCBM outperforms SAC (2-layer) by a large margin even in relatively dense reward settings like cheetah-run and walker-run. This margin becomes even larger in sparse-reward settings like walker-run-sparse and hopper-hop. Even comparing with SAC using 3-layer MLP, CTRL-UCBM also achieves better performance. This again shows the effectiveness of our method.</p>

    <h3 class="title">Offline Reinforcement Learning</h3>
    <img src="static/images/ctrl-table3.png" alt="MY ALT TEXT">
    <br>
    <p>We compare CTRL-LCB with state-of-the-art offline RL algorithms and include CPC as a representation learning baseline. Besides the state-action representation learned in CTRL-LCB, one major difference between CPC and CTRL-LCB is the additional reward penalty introduced by CTRL-LCB. The average normalized scores are presented in the table above. CTRL-LCB performs well on the medium-expert set of tasks. The medium set of tasks also benefit from our method. However, CTRL-LCB is unable to improve the performance of medium-replay, potentially due to low-quality data induced representations being incapable of recovering the good policy.</p>
</section>
</section>

<section class="section" id="muLV-Rep">
  <div class="container is-max-desktop content">
    <h2 class="title"><strong>III. &mu;LV-Rep</strong>: <span style="font-weight: normal;">Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning</span></h2>
    <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and  leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinforcement learning towards more practical applications.          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Introduction</h2>
      <div class="content">
        Empirical successes have motivated investigation into structured POMDPs that allow some of the core computational and statistical complexities to be overcome, and provides an improved understanding of exploitable structure and practical new algorithms with rigorous justification. However, most works rely on the existence of an ideal computational oracle for planning, which, unsurprisingly, is infeasible in most cases, hence difficult to apply in practice. Although there have been a few attempts to overcome the computational complexity of POMDPS, these algorithms are either only applicable to the tabular setting or rely on an integration oracle that quickly become intractable for large observation spaces. This gap immediately motivates the question:
    <div style="text-align: center; padding: 10px; margin-top: 20px; margin-bottom: 20px;" class="section hero is-light">
      <em> Can <strong>efficient</strong> and <strong>practical</strong> RL algorithms be designed for partial observations by exploiting natural structures?</em>
    </div>
</div>
By “efficient” we mean the statistical complexity avoids an exponential dependence on history length,
while by “practical” we mean that every component of learning, planning and exploration can be easily implemented and applied in practical settings.
<p>
In this paper, we provide an <strong>affirmative</strong> answer to this question. More specifically,
<ul>
  <li>We reveal for the first time that an &nbsp;-decodable POMDP admits a sufficient representation, the <em>Multi-step Latent Variable Representation (&mu;LV-Rep)</em>, that supports exact and tractable linear representation of the value functions, breaking the fundamental computational barriers;</li>
  <li>We design a computationally efficient planning algorithm that can implement both the principles of optimism and pessimism in the face of uncertainty for online and offline POMDPs respectively, by leveraging the learned sufficient representation &mu;LV-Rep;</li>
  <li>We provide a theoretical analysis of the sample complexity of the proposed algorithm, justifying its efficiency in balancing exploitation versus exploration;</li>
  <li>We conduct a comprehensive empirical comparison to current existing RL algorithms for POMDPs on several benchmarks, demonstrating the superior empirical performance of &mu;LV-Rep.</li>
</ul>
    <br>
    <h2 class="title">Experiments</h2>
    <br>
    We evaluate the proposed method on Meta-world, which is an open-source simulated benchmark consisting of 50 distinct robotic manipulation tasks with visual observations. We also provide experiment results on partial observable control problems constructed based on OpenAI gym MuJoCo.

    <h3 class="title">Mujoco Benchmarks</h3>
    <img src="static/images/mulvrep-table1.png" alt="MY ALT TEXT">
    <br>
    <p>Performance on various continuous control problems <span style="color: blue;">with partial observation</span>. All results are averaged across 4 random seeds and a window size of 10K. &mu;LV-Rep achieves the best performance compared to the baselines. Here, Best-FO denotes the performance of LV-Rep using <span style="color: red;">full observations</span> as inputs, providing a reference on how well an algorithm can achieve most in our tests.</p>
    <p>The results clearly demonstrate that the proposed method consistently delivers either competitive or superior outcomes across all domains  compared to both the model-based and model-free baselines. We note that in most domains, &mu;LV-Rep nearly matches the performance of Best-FO, further confirming that the proposed method is able to extract useful representations for decision-making in partially observable environments.  </p>

    <h3 class="title">Meta-world Benchmarks</h3>
    <img src="static/images/mulvrep-figure1.jpg" alt="MY ALT TEXT">
    <br>
    <p>Success Rate on 50 Meta-world tasks after 1 million interactions. The blue bars show our results, the gray bars show the best performance achieved by DreamerV2 and MWM. Our method succeeds on most of tasks (> 90% on 33 tasks). The results are better than or equivalent to (within 10% difference) the best of DreamerV2 and MWM on 41 tasks.</p>
    <p>It is noteworthy that while MWM achieves comparable sample efficiency with &mu;LV-Rep in certain tasks, it incurs higher computational costs and longer running times due to the incorporation of ViT network in its model. In our experimental configurations, &mu;LV-Rep demonstrates a training speed of 21.3 steps per second, outperforming MWM, which achieves a lower training speed of 8.1 steps per second. This highlights the computational efficiency of the proposed method.  </p>

</section>
</section>


<section class="section" id="Diff-SR">
  <div class="container is-max-desktop content">
    <h2 class="title"><strong>IV. Diff-SR</strong>: <span style="font-weight: normal;">Diffusion Spectral Representation for Reinforcement Learning</span></h2>
    <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Diffusion-based models have achieved notable empirical successes in reinforcement learning (RL) due to their expressiveness in modeling complex distributions. Despite existing methods being promising, the key challenge of extending existing methods for broader real-world applications lies in the computational cost at inference time, i.e., sampling from a diffusion model is considerably slow as it often requires tens to hundreds of iterations to generate even one sample. To circumvent this issue, we propose to leverage the flexibility of diffusion models for RL from a representation learning perspective. In particular, by exploiting the connection between diffusion model and energy-based model, we develop Diffusion Spectral Representation (Diff-SR), a coherent algorithm framework that enables extracting sufficient representations for value functions in Markov decision processes (MDP) and partially observable Markov decision processes (POMDP). We further demonstrate how Diff-SR facilitates efficient policy optimization and practical algorithms while explicitly bypassing the difficulty and inference cost of sampling from the diffusion model. Finally, we provide comprehensive empirical studies to verify the benefits of Diff-SR in delivering robust and advantageous performance across various benchmarks with both fully and partially observable settings.</div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Introduction</h2>
      <div class="content">
        Although diffusion models are powerful tools to represent complex, multimodal data distributions, its flexibility comes with a substantial inference cost: generating even a single sample from a diffusion model is notably slow, typically requiring tens to thousands of iterations. The computational demands are particularly problematic for RL, since the learning agent must frequently query the model for interactions with the environment. 
        <!-- Meanwhile, the planning with exploration issue has not been explicitly considered in the existing diffusion-based RL algorithms. The flexibility of diffusion models in fact induces extra difficulty in implementation of the principle of optimism in the face of uncertainty in planning step to balance the inherent trade-off between exploration vs. exploitation, which is indispensable in the online setting to avoid suboptimal policy.  -->
        <p>
        In this paper, we exploit the connection between diffusion models and energy-based models (EBMs), and develop Diffusion Spectral Representation (Diff-SR), which leverages the flexibility of diffusion models by extracting representations that capture the dynamics structure. Specifically,
        <ul>
        <li>We show that such diffusion-based representations are sufficiently expressive to represent the value function of any policy, which paves the way for efficient planning and exploration;</li>
        <li>We circumvent the need for sample generation from the diffusion model, and thus avoiding the inference costs associated with prior diffusion-based methods;</li>
        <li>Integrated with existing model-free policy optimization algorithms, Diff-SR demonstrates robust, superior performance and efficiency across various benchmarks.</li>
        </ul>
        <br>
        <h2 class="title">Experiments</h2>
        We evaluate Diff-SR with state-based MDP and image-based POMDP tasks to investigate the capability of Diff-SR comprehensively. 

        <h3 class="title">Gym-MuJoCo Benchmarks</h3>
        The following table presents the performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Diff-SR achieves significantly better or comparable performance to all baseline methods, including a diffusion-based method PolyGRAD in nearly all of the tasks. 
        <img src="static/images/diffsr-state-mdp.png" alt="MY ALT TEXT">
        Besides its superior performance, Diff-SR does not require sampling from the diffusion model while still leverages the flexibility of diffusion models. This leads to 4x faster training speed compared to other diffusion-based RL methods, as we observed in our experiments (see the next figure). 
        <img src="static/images/diffsr-efficiency.png" alt="MY ALT TEXT">
        <h3 class="title">Meta-World Benchmarks</h2>
        As the most difficult setting, we evaluate Diff-SR with 8 visual-input tasks selected from the Meta-World Benchmark. We see that Diff-SR achieves a greater than 90% success rate for seven of the tasks, 4 more tasks than the second best baseline &mu;LV-Rep. Overall, Diff-SR exhibits superior performance, faster convergence speed, and stable optimization in most of the tasks compared to the baseline methods. 
        <img src="static/images/diffsr-image-pomdp.png">
</div>


<!-- By “efficient” we mean the statistical complexity avoids an exponential dependence on history length,
while by “practical” we mean that every component of learning, planning and exploration can be easily implemented and applied in practical settings.
<p>
In this paper, we provide an <strong>affirmative</strong> answer to this question. More specifically,
<ul>
  <li>We reveal for the first time that an &nbsp;-decodable POMDP admits a sufficient representation, the <em>Multi-step Latent Variable Representation (&mu;LV-Rep)</em>, that supports exact and tractable linear representation of the value functions, breaking the fundamental computational barriers;</li>
  <li>We design a computationally efficient planning algorithm that can implement both the principles of optimism and pessimism in the face of uncertainty for online and offline POMDPs respectively, by leveraging the learned sufficient representation &mu;LV-Rep;</li>
  <li>We provide a theoretical analysis of the sample complexity of the proposed algorithm, justifying its efficiency in balancing exploitation versus exploration;</li>
  <li>We conduct a comprehensive empirical comparison to current existing RL algorithms for POMDPs on several benchmarks, demonstrating the superior empirical performance of &mu;LV-Rep.</li>
</ul>
    <br>
    <h2 class="title">Experiments</h2>
    <br>
    We evaluate the proposed method on Meta-world, which is an open-source simulated benchmark consisting of 50 distinct robotic manipulation tasks with visual observations. We also provide experiment results on partial observable control problems constructed based on OpenAI gym MuJoCo.

    <h3 class="title">Mujoco Benchmarks</h3>
    <img src="static/images/mulvrep-table1.png" alt="MY ALT TEXT">
    <br>
    <p>Performance on various continuous control problems <span style="color: blue;">with partial observation</span>. All results are averaged across 4 random seeds and a window size of 10K. &mu;LV-Rep achieves the best performance compared to the baselines. Here, Best-FO denotes the performance of LV-Rep using <span style="color: red;">full observations</span> as inputs, providing a reference on how well an algorithm can achieve most in our tests.</p>
    <p>The results clearly demonstrate that the proposed method consistently delivers either competitive or superior outcomes across all domains  compared to both the model-based and model-free baselines. We note that in most domains, &mu;LV-Rep nearly matches the performance of Best-FO, further confirming that the proposed method is able to extract useful representations for decision-making in partially observable environments.  </p>

    <h3 class="title">Meta-world Benchmarks</h3>
    <img src="static/images/mulvrep-figure1.jpg" alt="MY ALT TEXT">
    <br>
    <p>Success Rate on 50 Meta-world tasks after 1 million interactions. The blue bars show our results, the gray bars show the best performance achieved by DreamerV2 and MWM. Our method succeeds on most of tasks (> 90% on 33 tasks). The results are better than or equivalent to (within 10% difference) the best of DreamerV2 and MWM on 41 tasks.</p>
    <p>It is noteworthy that while MWM achieves comparable sample efficiency with &mu;LV-Rep in certain tasks, it incurs higher computational costs and longer running times due to the incorporation of ViT network in its model. In our experimental configurations, &mu;LV-Rep demonstrates a training speed of 21.3 steps per second, outperforming MWM, which achieves a lower training speed of 8.1 steps per second. This highlights the computational efficiency of the proposed method.  </p> -->

</section>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citations</h2>
      <pre><code>@misc{ren2023latent,
        title={Latent Variable Representation for Reinforcement Learning},
        author={Tongzheng Ren and Chenjun Xiao and Tianjun Zhang and Na Li and Zhaoran Wang and Sujay Sanghavi and Dale Schuurmans and Bo Dai},
        year={2023},
        eprint={2212.08765},
        archivePrefix={arXiv},
        primaryClass={cs.LG}
  }</code></pre>
  <pre><code>@misc{zhang2022making,
    title={Making Linear MDPs Practical via Contrastive Representation Learning},
    author={Tianjun Zhang and Tongzheng Ren and Mengjiao Yang and Joseph E. Gonzalez and Dale Schuurmans and Bo Dai},
    year={2022},
    eprint={2207.07150},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}</code></pre>
  <pre><code>@misc{zhang2024efficient,
      title={Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning},
      author={Hongming Zhang and Tongzheng Ren and Chenjun Xiao and Dale Schuurmans and Bo Dai},
      year={2024},
      eprint={2311.12244},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


 
  </body>
  </html>
